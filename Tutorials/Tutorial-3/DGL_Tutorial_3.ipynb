{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9geK3xNxyCp2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"cuda version: {torch.version.cuda}\")\n",
        "\n",
        "# Maks sure you're using the torch and cuda version\n",
        "# PyTorch 2.2: ${TORCH}=2.2.0 and ${CUDA}=cpu|cu118|cu121\n",
        "# PyTorch 2.1: ${TORCH}=2.1.0 and ${CUDA}=cpu|cu118|cu121\n",
        "# PyTorch 2.0: ${TORCH}=2.0.0 and ${CUDA}=cpu|cu117|cu118\n",
        "\n",
        "%env TORCH=2.1.0\n",
        "%env CUDA=cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liUV_37OMgdw"
      },
      "outputs": [],
      "source": [
        "%pip install -q dgl\n",
        "# %pip install -q torch_geometric\n",
        "%pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "# %pip install -q pyg_lib==0.3.1  -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "%pip install -q pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMOw1ccwrz7N"
      },
      "outputs": [],
      "source": [
        "import pyg_lib\n",
        "import torch_geometric\n",
        "\n",
        "print(f\"pyg-lib version: {pyg_lib.__version__}\")\n",
        "print(f\"torch-geometric version: {torch_geometric.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyPw-nLHIgYn"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "\n",
        "# Data handling, numerical processing, and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning, neural network modules, and metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Dropout\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import SAGEConv, GCNConv, global_mean_pool\n",
        "from torch_geometric.datasets import Planetoid, TUDataset\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.loader import GraphSAINTRandomWalkSampler, GraphSAINTNodeSampler, GraphSAINTEdgeSampler\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_sparse import SparseTensor\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, euclidean_distances\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Graph processing\n",
        "import networkx as nx\n",
        "\n",
        "# DGL and datasets\n",
        "from dgl.data import PPIDataset, CoraGraphDataset\n",
        "\n",
        "# Set a fixed random seed for reproducibility across multiple libraries\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check for CUDA (GPU support) and set device accordingly\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
        "    # Additional settings for ensuring reproducibility on CUDA\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA not available. Using CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnMhlDliqsk3"
      },
      "source": [
        "# Primer on sampling\n",
        "\n",
        "![img](https://i.ibb.co/fMkhkFF/Screenshot-2024-02-12-at-18-51-34.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t_BLXEtq44B"
      },
      "outputs": [],
      "source": [
        "def sampler(A, num_samples):\n",
        "  N = A.size(0)  # Number of nodes\n",
        "  sampled_neighbors = []\n",
        "\n",
        "  for i in range(N):\n",
        "    # Find indices of neighbors (non-zero elements) for node i\n",
        "    neighbors = torch.nonzero(A[i], as_tuple=False).squeeze(1)\n",
        "    #####################################################\n",
        "    ### TODO: sample nodes randomly\n",
        "\n",
        "    #####################################################\n",
        "    sampled_neighbors.append(samples.tolist())\n",
        "  return sampled_neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MruzaHXAuqY8"
      },
      "outputs": [],
      "source": [
        "adj = torch.tensor([\n",
        "    [0, 1, 1, 0, 1, 0],\n",
        "    [1, 0, 1, 0, 0, 0],\n",
        "    [1, 1, 0, 1, 1, 0],\n",
        "    [0, 0, 1, 0, 0, 1],\n",
        "    [1, 0, 1, 0, 0, 0],\n",
        "    [0, 0, 0, 1, 0, 0],\n",
        "], dtype=torch.float32, device=device)\n",
        "labels = torch.tensor([0, 0, 0, 1, 0, 1])\n",
        "\n",
        "sampler(adj, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hzn1EViCq44C"
      },
      "outputs": [],
      "source": [
        "# @title Plot graph utils\n",
        "def plot_graph(A, lbls, highlight_nodes=[], selected_node=None, ax=None):\n",
        "  graph = nx.from_numpy_array(A)\n",
        "\n",
        "  num_labels = np.unique(labels.numpy()).shape[0]\n",
        "  colormap = cm.get_cmap('viridis', num_labels)\n",
        "  colors = [colormap(i) for i in range(num_labels)]\n",
        "\n",
        "  label_color_map = {label:colors[i] for i, label in enumerate(np.unique(lbls))}\n",
        "\n",
        "  node_colors = [label_color_map[label] for label in lbls]\n",
        "  # default\n",
        "  default_node_color = 'lightblue'\n",
        "  default_node_size = 300\n",
        "  default_border_width = 1\n",
        "\n",
        "  # highlight\n",
        "  highlight_color = 'red'\n",
        "  highlight_size = 600\n",
        "  highlight_border_width = 3\n",
        "\n",
        "  pos = nx.spring_layout(graph, seed=0)\n",
        "  nx.draw_networkx_nodes(graph, pos,\n",
        "                        nodelist=[node for node in graph.nodes() if node not in highlight_nodes],\n",
        "                        node_color=[node_colors[i] for i, node in enumerate(graph.nodes()) if node not in highlight_nodes],\n",
        "                        node_size=default_node_size,\n",
        "                        linewidths=default_border_width,\n",
        "                        edgecolors='black', ax=ax)\n",
        "  nx.draw_networkx_nodes(graph, pos,\n",
        "                        nodelist=highlight_nodes,\n",
        "                        node_color=[node_colors[node] for i, node in enumerate(highlight_nodes)],\n",
        "                        node_size=highlight_size,\n",
        "                        linewidths=highlight_border_width,\n",
        "                        edgecolors='blue', ax=ax)\n",
        "  nx.draw_networkx_nodes(graph, pos,\n",
        "                        nodelist=[selected_node] if selected_node is not None else [],\n",
        "                        node_color=[selected_node] if selected_node is not None else [],\n",
        "                        node_size=highlight_size,\n",
        "                        linewidths=highlight_border_width-1,\n",
        "                        edgecolors='red', ax=ax)\n",
        "  nx.draw_networkx_edges(graph, pos, ax=ax)\n",
        "  nx.draw_networkx_labels(graph, pos, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aOjt1FY9q44D"
      },
      "outputs": [],
      "source": [
        "# @title plot graph\n",
        "selected_node = 2 # @param {type:\"integer\"}\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "\n",
        "# orginal graph\n",
        "ax = fig.add_subplot(2, 2, 1)\n",
        "ax.axis('off')\n",
        "ax.set_title(\"orginal graph\")\n",
        "plot_graph(adj.cpu().numpy(), labels.numpy(), ax=ax)\n",
        "\n",
        "# sampling neighbours\n",
        "for i in range(2, 5):\n",
        "  nodes = sampler(adj, num_samples=i)\n",
        "  ax = fig.add_subplot(2, 2, i)\n",
        "  ax.axis('off')\n",
        "  ax.set_title(f\"sample {i} nodes (rooted at node {selected_node})\")\n",
        "  plot_graph(adj.cpu().numpy(), labels.numpy(),\n",
        "             highlight_nodes=nodes[selected_node],\n",
        "             selected_node=selected_node,\n",
        "             ax=ax)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jpcuai_stfT"
      },
      "source": [
        "![img](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSt85ut3Pc21CJw8IZPDKJwlKe8Zw9W3u9K9FLYdo7Cjg&s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HROf8JBCq72S"
      },
      "outputs": [],
      "source": [
        "def random_walk(adj, start_node, walk_length):\n",
        "  # Ensure the adjacency matrix is a square matrix\n",
        "  assert adj.size(0) == adj.size(1), \"Adjacency matrix must be square.\"\n",
        "\n",
        "  num_nodes = adj.size(0)\n",
        "  walk = [start_node]\n",
        "\n",
        "  current_node = start_node\n",
        "  for _ in range(walk_length):\n",
        "    neighbors = adj[current_node].nonzero().view(-1)\n",
        "    if len(neighbors) == 0:\n",
        "        break  # If the current node has no neighbors, stop the walk\n",
        "    #####################################################\n",
        "    ### TODO: sample next walk (see torch.randint)\n",
        "\n",
        "    #####################################################\n",
        "    walk.append(next_node)\n",
        "    current_node = next_node\n",
        "\n",
        "  return walk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGCYdN0SrVZF"
      },
      "outputs": [],
      "source": [
        "random_walk(adj, 2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "28PC4f1arLSF"
      },
      "outputs": [],
      "source": [
        "# @title plot graph\n",
        "start_node = 2 # @param {type:\"integer\"}\n",
        "walk_length = 2 # @param {type:\"integer\"}\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "\n",
        "# orginal graph\n",
        "ax = fig.add_subplot(2, 2, 1)\n",
        "ax.axis('off')\n",
        "ax.set_title(\"orginal graph\")\n",
        "plot_graph(adj.cpu().numpy(), labels.numpy(), ax=ax)\n",
        "\n",
        "# sampling neighbours\n",
        "for i in range(2, 5):\n",
        "  nodes = random_walk(adj, start_node, i)\n",
        "  ax = fig.add_subplot(2, 2, i)\n",
        "  ax.axis('off')\n",
        "  ax.set_title(f\"perform {i} walks (rooted at node {start_node})\")\n",
        "  plot_graph(adj.cpu().numpy(), labels.numpy(),\n",
        "             highlight_nodes=nodes,\n",
        "             selected_node=start_node,\n",
        "             ax=ax)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt0RWaH9ILE_"
      },
      "source": [
        "# Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPz-6g1UxL4E"
      },
      "source": [
        "The PubMed dataset is used for node classification tasks.\n",
        "\n",
        "The dataset comprises scientific publications as nodes and citations between these publications as edges, forming a graph structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfcICDDkKmsC"
      },
      "outputs": [],
      "source": [
        "dataset = Planetoid(root='.', name=\"Pubmed\")\n",
        "data = dataset[0]\n",
        "\n",
        "# Print information about the dataset\n",
        "print(f'Dataset: {dataset}')\n",
        "print('-------------------')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of nodes: {data.x.shape[0]}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "# Print information about the graph\n",
        "print(f'\\nGraph:')\n",
        "print('------')\n",
        "print(f'Training nodes: {sum(data.train_mask).item()}')\n",
        "print(f'Evaluation nodes: {sum(data.val_mask).item()}')\n",
        "print(f'Test nodes: {sum(data.test_mask).item()}')\n",
        "print(f'Edges are directed: {data.is_directed()}')\n",
        "print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Graph has loops: {data.has_self_loops()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E_abAMmzMzI"
      },
      "outputs": [],
      "source": [
        "def plot_subgraph_batch(subgraph, title=\"\"):\n",
        "  subgraph_nx = to_networkx(subgraph, to_undirected=True)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "  nx.draw_networkx(subgraph_nx,\n",
        "        pos=nx.spring_layout(subgraph_nx, seed=10), with_labels=True,\n",
        "        node_size=150, node_color=subgraph.y, cmap=\"cool\", font_size=10\n",
        "      )\n",
        "  plt.title(title)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd5Cf-MtB0Ql"
      },
      "outputs": [],
      "source": [
        "# display a subgraph\n",
        "sage_sampler = NeighborLoader(\n",
        "    data=data,\n",
        "    num_neighbors=[3, 1],\n",
        "    batch_size=3,\n",
        "    input_nodes=data.train_mask,\n",
        ")\n",
        "\n",
        "subgraph_sage_sampler = next(iter(sage_sampler))\n",
        "plot_subgraph_batch(subgraph_sage_sampler, title=\"node sampling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVIX02ROf1NU"
      },
      "source": [
        "\n",
        "GraphSAINT (Graph Sampling Based Inductive Learning Method) is a novel graph neural network (GNN) training framework designed to efficiently handle large graphs.\n",
        "\n",
        "**Advantages:**\n",
        "- Full-Graph Training Limitations\n",
        "- Minibatch Training Issues\n",
        "\n",
        "**Graph Sampling:** GraphSAINT introduces several graph sampling techniques (node sampling, edge sampling, and random walk sampling) to generate smaller subgraphs for training. These sampling methods are designed to preserve the structural properties of the original graph, ensuring that the sampled subgraphs are representative.\n",
        "\n",
        "[![Screenshot-2024-02-12-at-18-10-44.png](https://i.postimg.cc/xdMHt3FP/Screenshot-2024-02-12-at-18-10-44.png)](https://postimg.cc/Jy7GhZ8y)\n",
        "\n",
        "**Normalization and Scaling:** To address the bias introduced by sampling, GraphSAINT proposes a normalization scheme that scales the loss and gradient for each sampled subgraph according to its sampling probability. This normalization ensures unbiased gradient estimates, leading to more effective and stable training.\n",
        "\n",
        "**Inductive Learning:** GraphSAINT is an inductive learning method, meaning it can generalize to unseen nodes and graphs.\n",
        "\n",
        "[![Screenshot-2024-02-12-at-18-10-57.png](https://i.postimg.cc/mkDvGwqv/Screenshot-2024-02-12-at-18-10-57.png)](https://postimg.cc/47DL6zXP)\n",
        "\n",
        "Zeng, Hanqing, et al. \"[Graphsaint: Graph sampling based inductive learning method.](https://arxiv.org/pdf/1907.04931.pdf)\" arXiv preprint arXiv:1907.04931 (2019)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InLUCHvruegn"
      },
      "outputs": [],
      "source": [
        "# @title pre-compute subgraphs\n",
        "def pre_load():\n",
        "  # compute once to pre-load sampled subgraphs in local dir\n",
        "  GraphSAINTRandomWalkSampler(data, batch_size=3000, walk_length=2,\n",
        "                                     num_steps=5, sample_coverage=100,\n",
        "                                     save_dir=dataset.processed_dir,\n",
        "                                     log=False, num_workers=4)\n",
        "pre_load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKO0rt4tmkI2"
      },
      "outputs": [],
      "source": [
        "# display a subgraph\n",
        "saint_walker_sampler = GraphSAINTRandomWalkSampler(data, batch_size=3, walk_length=2,\n",
        "                                     num_steps=5, sample_coverage=100,\n",
        "                                     save_dir=dataset.processed_dir,\n",
        "                                     log=False, num_workers=4)\n",
        "subgraph_saint_walker_sampler = next(iter(saint_walker_sampler))\n",
        "plot_subgraph_batch(subgraph_saint_walker_sampler, title=\"GraphSaint (random walk)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGhcJIdAkOMI"
      },
      "source": [
        "FastGCN introduces an efficient training method based on importance sampling, which significantly reduces the computational complexity and memory requirements. The key components of FastGCN include:\n",
        "\n",
        "- **Importance Sampling:** FastGCN proposes a layer-wise importance sampling technique that selects a subset of nodes for the convolution operation at each layer independently. By sampling nodes according to their importance scores, FastGCN ensures that the most informative nodes are more likely to be selected for training, reducing the number of computations needed while preserving the model's performance.\n",
        "\n",
        "- **Layer-wise Independence:** Unlike traditional GCNs, where the computation of each layer depends on the output of the previous layer, FastGCN's sampling strategy allows for independent sampling at each layer.\n",
        "\n",
        "![img](https://i.ibb.co/qDbw2k2/Screenshot-2024-02-12-at-18-42-00.png)\n",
        "\n",
        "\n",
        "Chen, Jie, Tengfei Ma, and Cao Xiao. \"[Fastgcn: fast learning with graph convolutional networks via importance sampling.](https://arxiv.org/pdf/1801.10247.pdf)\" arXiv preprint arXiv:1801.10247 (2018)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGAA1O_YAILk"
      },
      "outputs": [],
      "source": [
        "class FastGCNSampler:\n",
        "    def __init__(self, data, layer_sizes):\n",
        "        self.data = data\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.device = data.edge_index.device\n",
        "\n",
        "        # Compute normalization probabilities for nodes\n",
        "        edge_index = data.edge_index\n",
        "        num_nodes = data.num_nodes\n",
        "        row, col = edge_index\n",
        "        deg = torch_geometric.utils.degree(col, num_nodes)\n",
        "        self.adj = SparseTensor(\n",
        "            row=data.edge_index[0], col=data.edge_index[1],\n",
        "            value=torch.arange(data.num_edges, device=self.device),\n",
        "            sparse_sizes=(num_nodes, num_nodes))\n",
        "\n",
        "        self.probs = deg / deg.sum()\n",
        "\n",
        "    def sample(self, batch_nodes):\n",
        "        supports = []\n",
        "        sampled_nodes = batch_nodes\n",
        "        for size in self.layer_sizes:\n",
        "            support, sampled = self._sample_layer(sampled_nodes, size)\n",
        "            supports.append(support)\n",
        "            sampled_nodes = sampled\n",
        "        features = self.data.x[sampled].to(self.device)\n",
        "        return Data(x=features, edge_indices=supports[::-1], y=self.data.y[sampled],\n",
        "                    train_mask=self.data.train_mask[sampled],\n",
        "                    val_mask=self.data.val_mask[sampled],\n",
        "                    test_mask=self.data.test_mask[sampled],\n",
        "                    )\n",
        "\n",
        "    def _sample_layer(self, nodes, size):\n",
        "        # Sample nodes based on probabilities\n",
        "        support = self.adj[nodes, :]\n",
        "        neis = support.sum(0).nonzero().squeeze()\n",
        "        p1 = self.probs[neis]\n",
        "        p1 = p1 / torch.sum(p1)\n",
        "        #####################################################\n",
        "        ### TODO: sample new nodes according with probability p with replacement\n",
        "\n",
        "        #####################################################\n",
        "        u_sampled = neis[sampled]\n",
        "        support = support[:, u_sampled]\n",
        "        sampled_p1 = p1[sampled]\n",
        "\n",
        "        row, col, edge_idx = support.coo()\n",
        "        support = torch.stack([row, col], dim=0)\n",
        "\n",
        "        # Sample nodes based on probabilities\n",
        "        # sampled = torch.multinomial(self.probs, num_samples=size, replacement=True)\n",
        "        # Create subgraph for sampled nodes\n",
        "        # support = torch_geometric.utils.subgraph(sampled, self.data.edge_index, relabel_nodes=True, num_nodes=self.data.num_nodes)[0]\n",
        "\n",
        "        return support, sampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5HMXNscrnxy"
      },
      "outputs": [],
      "source": [
        "fastgcn_sampler = FastGCNSampler(data, [7, 9])\n",
        "subgraph_fastgcn_sampler = fastgcn_sampler.sample([0, 4])\n",
        "\n",
        "subgraph_fastgcn_sampler.edge_index = subgraph_fastgcn_sampler.edge_indices[0]\n",
        "plot_subgraph_batch(subgraph_fastgcn_sampler, title=\"FastGCN (layer-2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2Rz9iPfz0Iu"
      },
      "outputs": [],
      "source": [
        "subgraph_fastgcn_sampler.edge_index = subgraph_fastgcn_sampler.edge_indices[1]\n",
        "plot_subgraph_batch(subgraph_fastgcn_sampler, title=\"FastGCN (layer-1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6_j7Q05LMhN"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import degree, to_undirected\n",
        "from collections import Counter\n",
        "\n",
        "def plot_degree(data, ax=ax, title=\"\"):\n",
        "  edge_index = to_undirected(data.edge_index)\n",
        "  degrees = degree(edge_index[0]).numpy()\n",
        "  numbers = Counter(degrees)\n",
        "  degrees = list(map(int, numbers.keys()))\n",
        "  ax.set_title(title)\n",
        "  ax.set_xlabel('Node degree')\n",
        "  ax.set_ylabel('Number of nodes')\n",
        "  ax.bar(degrees, numbers.values())\n",
        "\n",
        "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(25, 10))\n",
        "plot_degree(data, ax=ax1, title=\"full graph\")\n",
        "plot_degree(subgraph_sage_sampler, ax=ax2, title=\"node sampler\")\n",
        "plot_degree(subgraph_saint_walker_sampler, ax=ax3, title=\"graph saint (random walk)\")\n",
        "ax4.set_axis_off()\n",
        "subgraph_fastgcn_sampler.edge_index = subgraph_fastgcn_sampler.edge_indices[0]\n",
        "plot_degree(subgraph_fastgcn_sampler, ax=ax5, title=\"fastgcn (layer 2)\")\n",
        "subgraph_fastgcn_sampler.edge_index = subgraph_fastgcn_sampler.edge_indices[1]\n",
        "plot_degree(subgraph_fastgcn_sampler, ax=ax6, title=\"fastgcn (layer 1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sPNY2Rcgqs3"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7s7-lNEoxUA"
      },
      "outputs": [],
      "source": [
        "def get_batches(train_ind, batch_size=64, shuffle=True):\n",
        "    nums = train_ind.shape[0]\n",
        "    if shuffle:\n",
        "        np.random.shuffle(train_ind)\n",
        "    i = 0\n",
        "    while i < nums:\n",
        "        cur_ind = train_ind[i:i + batch_size]\n",
        "        yield cur_ind\n",
        "        i += batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2gdPN0EP4ii"
      },
      "outputs": [],
      "source": [
        "def train_step(model, data , optimizer, criterion, model_type=\"\"):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  if model_type == \"fastgcn\":\n",
        "    out, _ = model(data.x, data.edge_indices)\n",
        "  else:\n",
        "    out, _ = model(data.x, data.edge_index)\n",
        "  loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.item(), out\n",
        "\n",
        "def train(model, loader, epochs, lr, num_edges=0, model_type=\"\", log_loss=False):\n",
        "  t = time.time()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),\n",
        "                            lr=lr, weight_decay=5e-4)\n",
        "  # criterion = torch.nn.CrossEntropyLoss()\n",
        "  criterion = F.nll_loss\n",
        "\n",
        "  best_model = None\n",
        "  best_valid_acc = 0\n",
        "\n",
        "  losses = []\n",
        "  for epoch in range(1, 1 + epochs):\n",
        "    train_loss = train_acc = valid_loss = valid_acc = 0\n",
        "    num_iters = 0\n",
        "    if model_type == \"fastgcn\":\n",
        "      iterator_ = get_batches(np.arange(num_edges))\n",
        "    else:\n",
        "      iterator_ = loader\n",
        "    for batch in iterator_:\n",
        "      if model_type == \"fastgcn\":\n",
        "        batch = loader.sample(batch)\n",
        "      batch = batch.to(device)\n",
        "      if batch.train_mask.sum() <= 0 or batch.y[batch.val_mask].sum() <= 0:\n",
        "        continue\n",
        "      loss, out = train_step(model, batch, optimizer, criterion, model_type)\n",
        "      train_loss += loss\n",
        "      train_acc += accuracy(batch.y[batch.train_mask],\n",
        "                            out[batch.train_mask].argmax(dim=1))\n",
        "\n",
        "      valid_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask])\n",
        "      valid_acc += accuracy(batch.y[batch.val_mask],\n",
        "                            out[batch.val_mask].argmax(dim=1))\n",
        "      num_iters += 1\n",
        "    train_loss /= num_iters\n",
        "    train_acc /= num_iters\n",
        "    valid_loss /= num_iters\n",
        "    valid_acc /= num_iters\n",
        "\n",
        "    if log_loss:\n",
        "      losses.append(train_loss)\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "    if(epoch % 10 == 0):\n",
        "      print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {train_loss:.4f}, ',\n",
        "          f'acc_train: {100*train_acc:.4f}%, ',\n",
        "          f'acc_valid: {100*valid_acc:.4f}%, ',\n",
        "          f'time: {time.time() - t:.4f}s.')\n",
        "  print(f'best acc_valid: {100*best_valid_acc:.4f}%')\n",
        "  if log_loss:\n",
        "    return best_model, losses\n",
        "  return best_model\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, metrics={}, model_type=\"\"):\n",
        "  model.eval()\n",
        "\n",
        "  # The output of model on all data\n",
        "  if model_type == \"fastgcn\":\n",
        "    logits, _ = model(data.x, [data.edge_index, data.edge_index])\n",
        "  else:\n",
        "    logits, _ = model(data.x, data.edge_index)\n",
        "  preds = logits.argmax(dim=1)\n",
        "\n",
        "  result = {\n",
        "        'train': {},\n",
        "        'val': {},\n",
        "        'test': {}\n",
        "      }\n",
        "\n",
        "  for name, metric in metrics.items():\n",
        "    result['train'][name] = metric(data.y[data.train_mask], preds[data.train_mask])\n",
        "    result['val'][name] = metric(data.y[data.val_mask], preds[data.val_mask])\n",
        "    result['test'][name] = metric(data.y[data.test_mask], preds[data.test_mask])\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ2cxwHyQzBM"
      },
      "outputs": [],
      "source": [
        "def calculate_specificity(y_true, y_pred, labels):\n",
        "    specificity_scores = np.zeros(len(labels))\n",
        "    for i, label in enumerate(labels):\n",
        "        binary_true = (y_true == label).int()\n",
        "        binary_pred = (y_pred == label).int()\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(binary_true, binary_pred).ravel()\n",
        "\n",
        "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "        specificity_scores[i] = specificity\n",
        "\n",
        "    return np.mean(specificity_scores)\n",
        "\n",
        "def compute_accuracy(target, prediction):\n",
        "  correct = (target == prediction).sum().item()\n",
        "  total = target.size(0)\n",
        "  accuracy = correct / total\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "sensitivity = lambda y_true, y_pred: recall_score(y_true.cpu(), y_pred.cpu(), average='macro')\n",
        "specificity = lambda y_true, y_pred: calculate_specificity(y_true.cpu(), y_pred.cpu(), labels.unique().cpu())\n",
        "accuracy = lambda y_true, y_pred: compute_accuracy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKtiFYhQ4Odj"
      },
      "outputs": [],
      "source": [
        "class GNN(torch.nn.Module):\n",
        "  def __init__(self, dim_in, dim_h, dim_out):\n",
        "    super().__init__()\n",
        "    self.gcn1 = GCNConv(dim_in, dim_h)\n",
        "    self.gcn2 = GCNConv(dim_h, dim_out)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    h = torch.relu(self.gcn1(x, edge_index))\n",
        "    h = F.dropout(h, p=0.5, training=self.training)\n",
        "    h = self.gcn2(h, edge_index)\n",
        "    return F.log_softmax(h, dim=1), h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1brdYYIgt3M"
      },
      "source": [
        "## Node Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nJsisRJ4MsK"
      },
      "outputs": [],
      "source": [
        "node_sampler = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[5, 10],\n",
        "    batch_size=16,\n",
        "    input_nodes=data.train_mask,\n",
        ")\n",
        "gnn_node_sampler = GNN(dataset.num_features, 64, dataset.num_classes)\n",
        "best_gnn_node_sampler = train(gnn_node_sampler, node_sampler, 50, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6A2qdvPYKmC"
      },
      "outputs": [],
      "source": [
        "metrics = {\"acc\": accuracy, \"sensitivity\": sensitivity, \"specifity\": specificity}\n",
        "results_node_sampling = test(best_gnn_node_sampler, data, metrics)\n",
        "results_node_sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL1t6jJihJEE"
      },
      "source": [
        "## Graph Saint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0iqZb4TkkHj"
      },
      "outputs": [],
      "source": [
        "saint_walker_sampler = GraphSAINTRandomWalkSampler(data, batch_size=200, walk_length=2,\n",
        "                                     num_steps=5, sample_coverage=100,\n",
        "                                     save_dir=dataset.processed_dir,\n",
        "                                     log=False, num_workers=4)\n",
        "gnn_rw_sampler = GNN(dataset.num_features, 64, dataset.num_classes)\n",
        "best_gnn_rw_sampler = train(gnn_rw_sampler, saint_walker_sampler, 100, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo7MPZZ-C5H2"
      },
      "outputs": [],
      "source": [
        "metrics = {\"acc\": accuracy, \"sensitivity\": sensitivity, \"specifity\": specificity}\n",
        "results_graph_saint = test(best_gnn_rw_sampler, data, metrics)\n",
        "results_graph_saint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOj6Gf2HDdbv"
      },
      "source": [
        "## FastGCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzuOLMLxVOOi"
      },
      "outputs": [],
      "source": [
        "class GNN(torch.nn.Module):\n",
        "  def __init__(self, dim_in, dim_h, dim_out):\n",
        "    super().__init__()\n",
        "    self.gcn1 = GCNConv(dim_in, dim_h, normalize=True)\n",
        "    self.gcn2 = GCNConv(dim_h, dim_out, normalize=True)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    h = torch.relu(self.gcn1(x, edge_index[0]))\n",
        "    h = F.dropout(h, p=0.5, training=self.training)\n",
        "    h = self.gcn2(h, edge_index[1])\n",
        "    return F.log_softmax(h, dim=1), h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71x2qz_dk4el"
      },
      "outputs": [],
      "source": [
        "fastgcn_sampler = FastGCNSampler(data, [64, 64])\n",
        "gnn_fgcn_sampler = GNN(dataset.num_features, 64, dataset.num_classes)\n",
        "best_gnn_fgcn_sampler = train(gnn_fgcn_sampler, fastgcn_sampler, 100, lr=0.01, num_edges=data.num_nodes, model_type=\"fastgcn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGExpiY8pSnP"
      },
      "outputs": [],
      "source": [
        "metrics = {\"acc\": accuracy, \"sensitivity\": sensitivity, \"specifity\": specificity}\n",
        "results_fastgcn = test(best_gnn_fgcn_sampler, data, metrics, model_type=\"fastgcn\")\n",
        "results_fastgcn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXeMtheaApw3"
      },
      "source": [
        "## Plot metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvA4CORVBGFp"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(metrics, title, phases=['train', 'val', 'test'], names=[]):\n",
        "    n_phases = len(phases)\n",
        "    if not names:\n",
        "      names = [\"\"] * len(metrics)\n",
        "    # Setting up the subplot grid\n",
        "    fig, axs = plt.subplots(n_phases, 1, figsize=(15, 5 * n_phases), sharey=False)\n",
        "\n",
        "    for i, phase in enumerate(phases):\n",
        "        # Extracting metric names from the first phase of the first metrics set\n",
        "        metric_names = list(metrics[0][phase].keys())\n",
        "\n",
        "        # Number of metrics\n",
        "        n_metrics = len(metric_names)\n",
        "\n",
        "        # Data for plotting: reshape scores for all models\n",
        "        scores_all_models = np.array([[metrics[model_idx][phase][metric] for metric in metric_names] for model_idx in range(len(metrics))])\n",
        "\n",
        "        # Setting the positions for the bars\n",
        "        pos = np.arange(n_metrics)\n",
        "        bar_width = 0.35 / len(metrics)  # Adjust bar width based on the number of models\n",
        "\n",
        "        # Plotting bars for each metric across models\n",
        "        for model_idx, (scores_model, name) in enumerate(zip(scores_all_models, names)):\n",
        "            axs[i].bar(pos + model_idx * bar_width, scores_model, bar_width, label=f'{names[model_idx]}')\n",
        "\n",
        "        # Adding labels and titles\n",
        "        axs[i].set_ylabel('Score')\n",
        "        axs[i].set_xlabel('Metrics')\n",
        "        axs[i].set_title(f'{phase.capitalize()} Phase Metrics')\n",
        "        axs[i].set_xticks(pos + bar_width / 2 * (len(metrics) - 1))\n",
        "        axs[i].set_xticklabels(metric_names)\n",
        "        axs[i].legend()\n",
        "\n",
        "    # Setting the main title and showing the plot\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rNXhWsQApFX"
      },
      "outputs": [],
      "source": [
        "plot_metrics([\n",
        "    results_node_sampling,\n",
        "    results_graph_saint,\n",
        "    results_fastgcn\n",
        "    ], 'Metric Comparison', names=[\"SAGE\", \"Saint\", \"FastGCN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2QNiUcoAXkR"
      },
      "source": [
        "# Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-KsS7sTuYte"
      },
      "outputs": [],
      "source": [
        "def plot_weight_distributions(weights1, weights2, labels, title=\"\"):\n",
        "    # Flatten the weight matrices\n",
        "    flattened_weights1 = weights1.flatten()\n",
        "    flattened_weights2 = weights2.flatten()\n",
        "\n",
        "    # Create KDE plots\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.kdeplot(flattened_weights1, fill=True, color=\"r\", label=labels[0])\n",
        "    sns.kdeplot(flattened_weights2, fill=True, color=\"b\", label=labels[1])\n",
        "\n",
        "    plt.title(f\"Distribution of GNN Weights {title}\")\n",
        "    plt.xlabel(\"Weight Value\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFjeyvKbuAr0"
      },
      "source": [
        "![img](https://i.ibb.co/XCZ6wSC/Screenshot-2024-02-12-at-19-07-11.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT64UV9PETRC"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super(BatchNorm, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        # Trainable parameters\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "        # Running mean and variance for inference\n",
        "        # self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        # self.register_buffer('running_var', torch.ones(num_features))\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            # Calculate mean and variance from input\n",
        "            batch_mean = torch.mean(x, dim=0)\n",
        "            batch_var = torch.var(x, dim=0, unbiased=False)\n",
        "\n",
        "            # Update running mean and variance\n",
        "            # self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            # self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "            # Normalize\n",
        "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var) + self.eps\n",
        "        else:\n",
        "            # Normalize using running mean and variance during inference\n",
        "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
        "\n",
        "        #####################################################\n",
        "        ### TODO: implement Scale and shift\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdaCZ4X9LL5o"
      },
      "outputs": [],
      "source": [
        "class GNNNoBatchNorm(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GNNNoBatchNorm, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1), x\n",
        "\n",
        "class GNNBatchNorm(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GNNBatchNorm, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.bn2 = BatchNorm(out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.bn1(self.conv1(x, edge_index))\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1), x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JNckRUyKw2v"
      },
      "outputs": [],
      "source": [
        "sampler = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[5, 10],\n",
        "    batch_size=300,\n",
        "    input_nodes=data.train_mask,\n",
        ")\n",
        "gnn_no_batchnorm = GNNNoBatchNorm(dataset.num_features, 64, dataset.num_classes)\n",
        "best_gnn_no_batchnorm, loss_no_batchnorm = train(gnn_no_batchnorm, sampler, 50, lr=0.01, log_loss=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl7056GLMx0f"
      },
      "outputs": [],
      "source": [
        "gnn_batchnorm = GNNBatchNorm(dataset.num_features, 64, dataset.num_classes)\n",
        "best_gnn_batchnorm, loss_batchnorm = train(gnn_batchnorm, sampler, 50, lr=0.01, log_loss=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXATJ8vUTjNR"
      },
      "outputs": [],
      "source": [
        "plot_weight_distributions(\n",
        "    gnn_no_batchnorm.conv1.lin.weight.detach().cpu().numpy(),\n",
        "    gnn_batchnorm.conv1.lin.weight.detach().cpu().numpy(),\n",
        "    labels=[\"No BatchNorm\", \"BatchNorm\"],\n",
        "    title=\"layer 1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIsgBIJBTkt9"
      },
      "outputs": [],
      "source": [
        "plot_weight_distributions(\n",
        "    gnn_no_batchnorm.conv2.lin.weight.detach().cpu().numpy(),\n",
        "    gnn_batchnorm.conv2.lin.weight.detach().cpu().numpy(),\n",
        "    labels=[\"No BatchNorm\", \"BatchNorm\"],\n",
        "    title=\"layer 2\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM0GU_8eUpS6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(loss_batchnorm, label='With BN')\n",
        "plt.plot(loss_no_batchnorm, label='Without BN')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bseyu01_U8Gx"
      },
      "source": [
        "# Inductive learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi27oH9oxuKp"
      },
      "source": [
        "![img](https://i.ibb.co/gdyGdLK/Screenshot-2024-02-12-at-19-23-42.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM3p0KrIx51q"
      },
      "source": [
        "Remember Global mean-pooling from lecture 3.3\n",
        "$$h_G=\\text{Mean}\\left(\\{h_K^i\\in\\mathbb{R}^{d_k}, \\forall V_G\\}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idP9QzK8XfRM"
      },
      "outputs": [],
      "source": [
        "class GNN(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "    super(GNN, self).__init__()\n",
        "    self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "    self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "    self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    x = F.relu(self.conv1(x, edge_index))\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = global_mean_pool(x, batch)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.lin(x)\n",
        "    return F.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc3aDuMMV86f"
      },
      "outputs": [],
      "source": [
        "dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
        "data = dataset[0]\n",
        "\n",
        "# Print information about the dataset\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('-------------------')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "\n",
        "print('------')\n",
        "print(data)\n",
        "print('------')\n",
        "\n",
        "# Gather some statistics about the a graph.\n",
        "print(f'\\nGraph:')\n",
        "print('------')\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vel6fbLecxVk"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into a training set and a test set\n",
        "train_dataset = dataset[:int(len(dataset) * 0.8)]\n",
        "test_dataset = dataset[int(len(dataset) * 0.8):]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IlFvupObpow"
      },
      "outputs": [],
      "source": [
        "# Select two graphs\n",
        "graph1 = dataset[0]\n",
        "graph2 = dataset[1]\n",
        "\n",
        "# Convert to NetworkX graphs\n",
        "G1 = to_networkx(graph1, to_undirected=True)\n",
        "G2 = to_networkx(graph2, to_undirected=True)\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot graph1\n",
        "axs[0].set_title('Molecule 1')\n",
        "nx.draw(G1, ax=axs[0], node_size=20, with_labels=False, node_color='skyblue')\n",
        "\n",
        "# Plot graph2\n",
        "axs[1].set_title('Molecule 2')\n",
        "nx.draw(G2, ax=axs[1], node_size=20, with_labels=False, node_color='lightgreen')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8BTXZiHV6qP"
      },
      "outputs": [],
      "source": [
        "model = GNN(dataset.num_node_features, 64, dataset.num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    data = data.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index, data.batch)\n",
        "    loss = criterion(out, data.y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "@torch.no_grad\n",
        "def test(loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  for data in loader:\n",
        "    data = data.to(device)\n",
        "    out = model(data.x, data.edge_index, data.batch)\n",
        "    pred = out.argmax(dim=1)\n",
        "    correct += int((pred == data.y).sum())\n",
        "  return correct / len(loader.dataset)\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "  train()\n",
        "  train_acc = test(train_loader)\n",
        "  test_acc = test(test_loader)\n",
        "  if epoch % 10 == 0:\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISmfoj4Ui8ZZ"
      },
      "source": [
        "# Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTT08Zx-vwb_"
      },
      "source": [
        "Train with PubMed on the full dataset to perform transductive learning.\n",
        "Remember, in transductive learning, the model learns from the entire graph, including both labeled and unlabeled nodes, but only predicts labels for the unlabeled nodes.\n",
        "\n",
        "1. write your training loops\n",
        "2. test the performance\n",
        "\n",
        "Answer the following questions:\n",
        "- is it faster?\n",
        "- does it perform better than sampling methods?\n",
        "\n",
        "Feel free to modify the GNN provided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRshrGpKv-50"
      },
      "outputs": [],
      "source": [
        "dataset = Planetoid(root='.', name=\"Pubmed\")\n",
        "data = dataset[0]\n",
        "\n",
        "# Print information about the dataset\n",
        "print(f'Dataset: {dataset}')\n",
        "print('-------------------')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of nodes: {data.x.shape[0]}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "# Print information about the graph\n",
        "print(f'\\nGraph:')\n",
        "print('------')\n",
        "print(f'Training nodes: {sum(data.train_mask).item()}')\n",
        "print(f'Evaluation nodes: {sum(data.val_mask).item()}')\n",
        "print(f'Test nodes: {sum(data.test_mask).item()}')\n",
        "print(f'Edges are directed: {data.is_directed()}')\n",
        "print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Graph has loops: {data.has_self_loops()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86d5zItpwR2V"
      },
      "outputs": [],
      "source": [
        "class GNN(torch.nn.Module):\n",
        "  def __init__(self, dim_in, dim_h, dim_out):\n",
        "    super().__init__()\n",
        "    self.gcn1 = GCNConv(dim_in, dim_h)\n",
        "    self.gcn2 = GCNConv(dim_h, dim_out)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    h = torch.relu(self.gcn1(x, edge_index))\n",
        "    h = F.dropout(h, p=0.5, training=self.training)\n",
        "    h = self.gcn2(h, edge_index)\n",
        "    return F.log_softmax(h, dim=1), h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XjnHDTIfskZ"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "### TODO: write your implementation in the following cells (feel free to create new ones)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
