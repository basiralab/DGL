{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upr5kdH4NFHJ"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iRXOF_ENFHL",
    "outputId": "a700d610-3ece-452e-fa73-0e7d947dafba"
   },
   "outputs": [],
   "source": [
    "%pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeGVT7D2NFHM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# for reproduceability\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1BFWK6wNFHM"
   },
   "source": [
    "## 1) Graph Classification with Different Pooling Mechanisms\n",
    "In this section, we are going to work on graph classification using GCN with different pooling mechanisms. As you know, GCNs modify the node feature matrix $X \\in \\mathbb{R}^{N\\times d}$ by updating the embeddings of each node in the graph.\n",
    "\n",
    "For graph classification, we need to get a graph-level embedding vector $d \\in \\mathbb{R}^{d \\times 1}$. To do so, we need to **pool** node embedding vectors into a single vector.\n",
    "\n",
    "We are going look at different pooling mechanisms, namely:\n",
    "- Global Mean Pooling\n",
    "- Global Max Pooling\n",
    "- Global Sum Pooling\n",
    "\n",
    "![gc.jpg](figures/gc.jpg)\n",
    "\n",
    "\n",
    "### 1.1) Dataset\n",
    "The most common task for graph classification is **molecular property prediction**, in which molecules are represented as graphs, and the task may be to infer whether a molecule inhibits HIV virus replication or not.\n",
    "\n",
    "The TU Dortmund University has collected a wide range of different graph classification datasets, known as the [**TUDatasets**](https://chrsmrrs.github.io/datasets/), which are also accessible via [`torch_geometric.datasets.TUDataset`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset) in PyTorch Geometric.\n",
    "Let's load and inspect one of the smaller ones, the **MUTAG dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhNL_yzyNFHM"
   },
   "outputs": [],
   "source": [
    "mutag_dataset = TUDataset(root='data/TUDataset', name='MUTAG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGMUa9shNFHN"
   },
   "source": [
    "#### 1.1.1) Dataset Statistics\n",
    "Let's see the statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIb2D4DDNFHN",
    "outputId": "e3775b3b-a3f5-49ea-e032-1acd5b0b1f13"
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(f'Dataset: {mutag_dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(mutag_dataset)}')\n",
    "print(f'Number of features: {mutag_dataset.num_features}')\n",
    "print(f'Number of classes: {mutag_dataset.num_classes}')\n",
    "\n",
    "data = mutag_dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUCKm13Gt5WS"
   },
   "source": [
    "#### 1.1.2) Train-Test Split and Preprocessing\n",
    "PyG keeps the adj matrix in **COO format**, we convert it to $N\\times N$ adj matrix format using `to_dense_adj` utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZ4TrU2EqbL0",
    "outputId": "d8670344-3eb6-430e-f3ac-57896a44153e"
   },
   "outputs": [],
   "source": [
    "def preprocess_gc_dataset(dataset):\n",
    "    train_ds, test_ds = train_test_split(dataset, train_size=0.8, shuffle=True, random_state=seed)\n",
    "\n",
    "    # PyG keeps the adj matrix in COO format, we convert it to NxN adj matrix format using to_dense_adj utility function\n",
    "    train_ds = [(to_dense_adj(graph.edge_index).squeeze(), graph.x, graph.y) for graph in train_ds]\n",
    "    test_ds = [(to_dense_adj(graph.edge_index).squeeze(), graph.x, graph.y) for graph in test_ds]\n",
    "\n",
    "    return train_ds, test_ds\n",
    "\n",
    "train_ds, test_ds = preprocess_gc_dataset(mutag_dataset)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_ds)}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdel3XYiNFHO"
   },
   "source": [
    "#### 1.1.3) Visualization of Graphs\n",
    "Let's visualize one of the graphs using `networkx` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "VcKEBk6DNFHO",
    "outputId": "1c989858-4c8f-48e1-a34b-573280d1b748"
   },
   "outputs": [],
   "source": [
    "# MUTAG graphs are undirected,\n",
    "#Â so set to_undirected=True to visualize it as an undirected graph (default will be to visualize as directed graph)\n",
    "nx_graph = nx.from_numpy_array(train_ds[0][0].numpy())\n",
    "nx.draw(nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue2xcey_NFHP"
   },
   "source": [
    "### 1.2) GCN Model\n",
    "\n",
    "Remember from the Tutorial 1 that the feature update rule for the next layer $ H_{k+1} $ in a graph convolutional network is given by the equation\n",
    "\n",
    "$$\n",
    "H_{k+1} = \\sigma(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H_k \\Omega_k + \\beta)\n",
    "$$\n",
    "\n",
    "where  $\\tilde{D}$ is the degree matrix with added self-loops, $\\tilde{A}$ is the adjacency matrix with self-loops, $H_k$ are the features from the previous layer, $ \\Omega_k $ is the weight matrix at layer $ k $, $\\beta$ is the bias term, and $\\sigma $ denotes the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnM0JtwnSFPQ"
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of a Graph Convolutional Network (GCN).\n",
    "    ...\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.Omega = nn.Parameter(torch.randn(input_dim, output_dim) * torch.sqrt(torch.tensor(2.0) / (input_dim + output_dim)))\n",
    "        self.beta = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, H_k, A_normalized):\n",
    "        agg = torch.matmul(A_normalized, H_k)\n",
    "        H_k_next = torch.matmul(agg, self.Omega) + self.beta\n",
    "        return H_k_next\n",
    "\n",
    "class GCN_GC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, pooling_fn):\n",
    "        super(GCN_GC, self).__init__()\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.conv1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNLayer(hidden_dim, num_classes)\n",
    "\n",
    "        self.pooling_fn = pooling_fn\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        # Apply self-connections\n",
    "        A = A + torch.eye(A.size(0))\n",
    "\n",
    "        # Apply normalization\n",
    "        epsilon = 1e-5  # Small constant to avoid division by zero\n",
    "        d = A.sum(1) + epsilon\n",
    "        D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "        A = D_inv @ A @ D_inv\n",
    "\n",
    "        # Pass through GCN layers\n",
    "        H1 = self.conv1(X, A)\n",
    "        H1 = F.relu(H1)\n",
    "        H2 = self.conv2(H1, A)\n",
    "\n",
    "        pooled = self.pooling_fn(H2)\n",
    "\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9c4KvNgNFHP"
   },
   "source": [
    "### 1.3) Global Mean Pooling\n",
    "![mean_pool.jpg](figures/mean_pool.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4WDwf3qNFHP"
   },
   "outputs": [],
   "source": [
    "def global_mean_pooling(X: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        X (tensor): Node feature matrix of shape (Nxd)\n",
    "    Returns:\n",
    "        Combined embeddings vector of shape (num_classes,)\n",
    "    \"\"\"\n",
    "\n",
    "    return X.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W_74SBtNFHP"
   },
   "source": [
    "### 1.4) Global Max Pooling\n",
    "![max_pool.jpg](figures/max_pool.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWrerDG1NFHP"
   },
   "outputs": [],
   "source": [
    "def global_max_pooling(X: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        X (tensor): Node feature matrix of shape (Nxd)\n",
    "    Returns:\n",
    "        Combined embeddings vector of shape (num_classes,)\n",
    "    \"\"\"\n",
    "\n",
    "    return X.max(dim=0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sayYu-38NFHQ"
   },
   "source": [
    "### 1.5) Global Sum Pooling\n",
    "![sum_pool.jpg](figures/sum_pool.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7ymiGBNNFHQ"
   },
   "outputs": [],
   "source": [
    "def global_sum_pooling(X: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        X (tensor): Node feature matrix of shape (Nxd)\n",
    "    Returns:\n",
    "        Combined embeddings vector of shape (num_classes,)\n",
    "    \"\"\"\n",
    "\n",
    "    return X.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTM9Zti0NFHQ"
   },
   "source": [
    "### 1.6) Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caXS_ms3NFHQ"
   },
   "outputs": [],
   "source": [
    "def train_epoch_gc(model, dataset, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for A, X, y in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(A, X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(output.view(1, -1), y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataset)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "def train_gc(model, train_ds, optimizer, loss_fn, num_epochs):\n",
    "    loss_values = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_epoch_gc(model, train_ds, optimizer, loss_fn)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "        loss_values.append(loss)\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_gc(model, dataset):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for A, X, y in dataset:\n",
    "        output = model(A, X)\n",
    "\n",
    "        predicted = output.argmax()\n",
    "\n",
    "        true_labels.append(y.cpu().item())\n",
    "        predicted_labels.append(predicted.cpu().item())\n",
    "\n",
    "    # Debug: Print true and predicted labels\n",
    "    print(\"True labels:\", true_labels)\n",
    "    print(\"Predicted labels:\", predicted_labels)\n",
    "\n",
    "    # Calculate accuracy, precision, sensitivity (recall), and F1-score\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    sensitivity = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    # Print out the evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    return accuracy, precision, sensitivity, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BijWZkT1wKF4",
    "outputId": "5a5bb361-7855-4746-8ae0-09e8576bbb21"
   },
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    \"Global Mean Pooling\": global_mean_pooling,\n",
    "    \"Global Max Pooling\": global_max_pooling,\n",
    "    \"Global Sum Pooling\": global_sum_pooling,\n",
    "}\n",
    "\n",
    "num_hidden = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "evaluation_metrics = {}\n",
    "train_loss_values = {}\n",
    "\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "\n",
    "def train_and_test_gc(pooling_name, pooling_fn):\n",
    "    print(f\"\\n==== {pooling_name} ====\\n\")\n",
    "\n",
    "    model = GCN_GC(mutag_dataset.num_features, num_hidden, mutag_dataset.num_classes, pooling_fn)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"\\nTraining:\\n\")\n",
    "    train_loss_values[pooling_name] = train_gc(model, train_ds, optimizer, loss_fn, num_epochs)\n",
    "\n",
    "    print(\"\\nTesting:\\n\")\n",
    "    evaluation_metrics[pooling_name] = test_gc(model, test_ds)\n",
    "\n",
    "# Train and test for each pooling configuration\n",
    "for pooling_name, pooling_fn in configurations.items():\n",
    "    train_and_test_gc(pooling_name, pooling_fn)\n",
    "\n",
    "# Plot training loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (pooling_name, loss_values) in enumerate(train_loss_values.items()):\n",
    "    plt.plot(range(1, num_epochs + 1), loss_values, label=pooling_name, linewidth=4, color=colors[i])\n",
    "\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "metrics_names = ['Accuracy', 'Precision', 'Sensitivity', 'F1-score']\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    axes[i].bar(\n",
    "        configurations.keys(),\n",
    "        [evaluation_metrics[key][i] for key in configurations],\n",
    "        color=['red', 'green', 'blue'],\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    axes[i].set_title(metric_name)\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].set_xticklabels(configurations.keys(), rotation=45, ha='right')\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKkMOtg7suLs"
   },
   "source": [
    "### 1.7) Adding a Learnable Classification Head\n",
    "Let's see what happens if we add a **fully-connected classification layer** on top of the convolution layers and pooling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMxp_S84y26X"
   },
   "outputs": [],
   "source": [
    "class GCN_GC_With_Classification_Head(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, pooling_fn):\n",
    "        super(GCN_GC_With_Classification_Head, self).__init__()\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.conv1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNLayer(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        self.pooling_fn = pooling_fn\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        # Apply self-connections\n",
    "        A = A + torch.eye(A.size(0))\n",
    "\n",
    "        # Apply normalization\n",
    "        epsilon = 1e-5  # Small constant to avoid division by zero\n",
    "        d = A.sum(1) + epsilon\n",
    "        D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "        A = D_inv @ A @ D_inv\n",
    "\n",
    "        # Pass through GCN layers\n",
    "        H1 = self.conv1(X, A)\n",
    "        H1 = F.relu(H1)\n",
    "        H2 = self.conv2(H1, A)\n",
    "\n",
    "        pooled = self.pooling_fn(H2)\n",
    "        fc_out = self.fc(pooled)\n",
    "\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hcHPG_l8thqu",
    "outputId": "1ba5da49-1195-4ead-cea7-93da7ceeb089"
   },
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    \"Global Mean Pooling\": (global_mean_pooling, False),\n",
    "    \"Global Mean Pooling + Linear Layer\": (global_mean_pooling, True),\n",
    "    \"Global Max Pooling\": (global_max_pooling, False),\n",
    "    \"Global Max Pooling + Linear Layer\": (global_max_pooling, True),\n",
    "    \"Global Sum Pooling\": (global_sum_pooling, False),\n",
    "    \"Global Sum Pooling + Linear Layer\": (global_sum_pooling, True),\n",
    "}\n",
    "\n",
    "num_hidden = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "evaluation_metrics = {}\n",
    "train_loss_values = {}\n",
    "\n",
    "colors = [\"red\", \"purple\", \"blue\", \"cyan\", \"green\", \"lime\"]\n",
    "\n",
    "def train_and_test_gc_with_lin(pooling_name, pooling_fn, linear_layer):\n",
    "    print(f\"\\n==== {pooling_name} ====\\n\")\n",
    "\n",
    "    if linear_layer:\n",
    "        model = GCN_GC_With_Classification_Head(mutag_dataset.num_features, num_hidden, mutag_dataset.num_classes, pooling_fn)\n",
    "    else:\n",
    "        model = GCN_GC(mutag_dataset.num_features, num_hidden, mutag_dataset.num_classes, pooling_fn)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"\\nTraining:\\n\")\n",
    "    train_loss_values[pooling_name] = train_gc(model, train_ds, optimizer, loss_fn, num_epochs)\n",
    "\n",
    "    print(\"\\nTesting:\\n\")\n",
    "    evaluation_metrics[pooling_name] = test_gc(model, test_ds)\n",
    "\n",
    "# Train and test for each pooling configuration\n",
    "for pooling_name, (pooling_fn, linear_layer) in configurations.items():\n",
    "    train_and_test_gc_with_lin(pooling_name, pooling_fn, linear_layer)\n",
    "\n",
    "# Plot training loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (pooling_name, loss_values) in enumerate(train_loss_values.items()):\n",
    "    plt.plot(range(1, num_epochs + 1), loss_values, label=pooling_name, linewidth=4, color=colors[i])\n",
    "\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "metrics_names = ['Accuracy', 'Precision', 'Sensitivity', 'F1-score']\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    axes[i].bar(\n",
    "        configurations.keys(),\n",
    "        [evaluation_metrics[key][i] for key in configurations],\n",
    "        color=colors,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    axes[i].set_title(metric_name)\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].set_xticklabels(configurations.keys(), rotation=45, ha='right')\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujJRjlNL2Tfz"
   },
   "source": [
    "## 2) Simple Graph Convolution (SGC)\n",
    "In this section, we are going to implement Simple Graph Convolution (SGC) model and compare it with GCN on a node-wise classification task.\n",
    "\n",
    "Simple Graph Convolution (SGC) was introduced by [Wu et al. (2019)](https://arxiv.org/pdf/1902.07153.pdf) as a **scalable** graph convolution architecture. They reduce the excess complexity of GCNs by repeatedly removing the nonlinearities between GCN layers and collapsing the resulting function into a **single linear transformation**.\n",
    "\n",
    "![sgc.jpg](figures/sgc.jpg)\n",
    "\n",
    "They hypothesize that the nonlinearity between GCN layers is not critical - but that the majority of the benefit arises from the local averaging. They therefore remove the nonlinear transition functions between each layer and only keep the final softmax (in order to obtain probabilistic outputs). The resulting model is linear, but still has the same increased âreceptive fieldâ of a K-layer GCN\n",
    "\n",
    "$$\\hat{\\mathbf{Y}} = softmax(\\mathbf{S}...\\mathbf{S}\\mathbf{S}\\mathbf{X}\\mathbf{\\Theta}^{(1)}\\mathbf{\\Theta}^{(2)}...\\mathbf{\\Theta}^{(K)})$$\n",
    "\n",
    "where $$\\mathbf{S} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$$\n",
    "\n",
    "To simplify notation, one can collapse the repeated multiplication with the normalized adjacency matrix S into a single matrix by raising S to the K-th power. Also, one can reparameterize the weights into a single matrix $\\mathbf{\\Theta}=\\mathbf{\\Theta}^{(1)}\\mathbf{\\Theta}^{(2)}...\\mathbf{\\Theta}^{(K)}$. The resulting classifier becomes\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}_{SGC} = softmax(\\mathbf{S}^K\\mathbf{X}\\mathbf{\\Theta})$$\n",
    "\n",
    "Furthermore, computation of $\\mathbf{S}^K\\mathbf{X}$ requires no weight, it is essentially equivalent to a feature\n",
    "pre-processing step and the entire training of the model reduces to straight-forward multi-class logistic regression on the pre-processed features. In short, we can **precompute** the neighborhood propagation matrix $\\mathbf{S}^K\\mathbf{X}$ and training of SGC simplifies into a **logistic regression training**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7HW_df7VpFA"
   },
   "source": [
    "### 2.1) Implementation of SGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqCnZdq7uzme"
   },
   "outputs": [],
   "source": [
    "class SGC(nn.Module):\n",
    "    \"\"\"\n",
    "    A Simple PyTorch Implementation of Logistic Regression.\n",
    "    Assuming the features have been preprocessed with k-step graph propagation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SGC, self).__init__()\n",
    "\n",
    "        self.W = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5o4LCnSVsSG"
   },
   "source": [
    "### 2.2) Precomputation of Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuEnLjg4SAgv"
   },
   "outputs": [],
   "source": [
    "def sgc_precompute(X, S, deg):\n",
    "    \"\"\"\n",
    "        Precomputes the k-step (=deg=degree) graph propagation.\n",
    "        Params:\n",
    "            X (tensor): Node feature matrix of shape Nxd\n",
    "            S (tensor): Normalized and self-looped adj. matrix of shape NxN\n",
    "            deg (int): Degree (or k) of propagation (k-step propagation)\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed features matrix of shape Nxd\n",
    "    \"\"\"\n",
    "\n",
    "    t = perf_counter()\n",
    "    for _ in range(deg):\n",
    "        X = torch.mm(S, X)\n",
    "    precompute_time = perf_counter()-t\n",
    "    return X, precompute_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fu_vIOryV5uA"
   },
   "source": [
    "### 2.3) Baseline GCN Implementation for Node Classifcation (NC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnW8tNqYTO6a"
   },
   "outputs": [],
   "source": [
    "class GCN_NC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(GCN_NC, self).__init__()\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.conv1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNLayer(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        H1 = self.conv1(X, A)\n",
    "        H1 = F.relu(H1)\n",
    "        H2 = self.conv2(H1, A)\n",
    "\n",
    "        return H2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "794AxVQqV_8q"
   },
   "source": [
    "### 2.4) Cora Dataset\n",
    "\n",
    "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
    "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
    "Two documents are connected if there exists a citation link between them.\n",
    "The task is to infer the category of each document (7 in total).\n",
    "\n",
    "This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n",
    "We again can make use [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) for an easy access to this dataset via [`torch_geometric.datasets.Planetoid`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwiskXKVTuIt"
   },
   "outputs": [],
   "source": [
    "cora_dataset = Planetoid(root='data/Planetoid', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOOM_yagWiUV",
    "outputId": "459813a0-7eea-4c22-c32d-5bafc9dc8b43"
   },
   "outputs": [],
   "source": [
    "def print_nc_dataset_stats(dataset):\n",
    "    print()\n",
    "    print(f'Dataset: {dataset}:')\n",
    "    print('======================')\n",
    "    print(f'Number of graphs: {len(dataset)}')\n",
    "    print(f'Number of features: {dataset.num_features}')\n",
    "    print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "    data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "    print()\n",
    "    print(data)\n",
    "    print('===========================================================================================================')\n",
    "\n",
    "    # Gather some statistics about the graph.\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "    print(f'Has self-loops: {data.has_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "print_nc_dataset_stats(cora_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzdBggSawYo3",
    "outputId": "b9d25823-4247-47d8-8fcf-dbe50d120685"
   },
   "outputs": [],
   "source": [
    "def preprocess_nc_dataset(dataset):\n",
    "    g = dataset[0]\n",
    "\n",
    "    # Extract features, labels\n",
    "    features = g.x\n",
    "    labels = g.y\n",
    "    A = to_dense_adj(g.edge_index).squeeze()\n",
    "\n",
    "    # Apply self-connections\n",
    "    A = A + torch.eye(A.size(0))\n",
    "\n",
    "    # Apply normalization\n",
    "    epsilon = 1e-5  # Small constant to avoid division by zero\n",
    "    d = A.sum(1) + epsilon\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    A = D_inv @ A @ D_inv\n",
    "\n",
    "    num_nodes = g.num_nodes\n",
    "    num_train = int(0.8 * num_nodes)  # 80% for training\n",
    "    num_test = int(0.2 * num_nodes)    # 20% for testing\n",
    "\n",
    "    print(f\"Number of training nodes: {num_train}\")\n",
    "    print(f\"Number of testing nodes: {num_test}\")\n",
    "\n",
    "    # Create a random permutation of node indices\n",
    "    indices = torch.randperm(num_nodes)\n",
    "\n",
    "    # Assign the first num_train nodes to the training set\n",
    "    # Assign the remaining nodes to the test set\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[indices[:num_train]] = True\n",
    "    test_mask[indices[:num_train]] = True\n",
    "\n",
    "    train_features = features[train_mask]\n",
    "    test_features = features[test_mask]\n",
    "\n",
    "    train_labels = labels[train_mask]\n",
    "    test_labels = labels[test_mask]\n",
    "\n",
    "    train_adj = A[train_mask][:, train_mask]\n",
    "    test_adj = A[test_mask][:, test_mask]\n",
    "\n",
    "    return train_features, test_features, train_labels, test_labels, train_adj, test_adj\n",
    "\n",
    "cora_train_X, cora_test_X, cora_train_y, cora_test_y, cora_train_A, cora_test_A = preprocess_nc_dataset(cora_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCFJBIqWWxRa"
   },
   "source": [
    "### 2.5) Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtC23mO2AFq4"
   },
   "outputs": [],
   "source": [
    "num_feats = cora_dataset.num_features\n",
    "num_classes = cora_dataset.num_classes\n",
    "hidden_dim = 64\n",
    "sgc_deg = 2\n",
    "num_epochs = 300\n",
    "lr = 0.2\n",
    "sgc_loss_values = []\n",
    "gcn_loss_values = []\n",
    "\n",
    "sgc_epoch_times = []\n",
    "gcn_epoch_times = []\n",
    "\n",
    "sgc = SGC(num_feats, num_classes)\n",
    "gcn = GCN_NC(num_feats, hidden_dim, num_classes)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "sgc_optimizer = optim.AdamW(sgc.parameters(), lr=lr)\n",
    "gcn_optimizer = optim.AdamW(gcn.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co9YU8QmW8M-"
   },
   "source": [
    "### 2.6) Precomputing Neighborhood Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V26P1Bl8Bivc",
    "outputId": "b24351aa-0dc8-4ef6-e856-b7d9c11d14b0"
   },
   "outputs": [],
   "source": [
    "cora_train_X_precomputed, precomputing_time = sgc_precompute(cora_train_X, cora_train_A, sgc_deg)\n",
    "print(f\"Precomputation time: {precomputing_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1HERBmyXJt4"
   },
   "source": [
    "### 2.7) Training of SGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS5FVYmfAgdX",
    "outputId": "78a730d3-fe2e-40a0-c441-f8ba4f878e3a"
   },
   "outputs": [],
   "source": [
    "print(\"=== TRAINING SGC ===\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Record the start time of the epoch\n",
    "    epoch_start_time = perf_counter()\n",
    "\n",
    "    sgc.train()\n",
    "    sgc_optimizer.zero_grad()\n",
    "    output = sgc(cora_train_X_precomputed)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(output, cora_train_y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    sgc_optimizer.step()\n",
    "\n",
    "    # Record the end time of the epoch\n",
    "    epoch_end_time = perf_counter()\n",
    "\n",
    "    # Calculate the time taken for the epoch\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # Store the epoch time in the list\n",
    "    sgc_epoch_times.append(epoch_time)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Time: {epoch_time:.4f} seconds\")\n",
    "    sgc_loss_values.append(loss)\n",
    "\n",
    "# Calculate the average time per epoch\n",
    "sgc_avg_time_per_epoch = sum(sgc_epoch_times) / len(sgc_epoch_times)\n",
    "sgc_training_time = sum(sgc_epoch_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6TFdr1PXQJk"
   },
   "source": [
    "### 2.8) Testing of SGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfSDdM0ICHDE"
   },
   "outputs": [],
   "source": [
    "cora_test_X_precomputed, time = sgc_precompute(cora_test_X, cora_test_A, sgc_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxwYyj2fB9W7"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sgc.eval()\n",
    "    output = sgc(cora_test_X_precomputed)\n",
    "\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "    correct = (predicted == cora_test_y).sum().item()\n",
    "    total = cora_test_y.size(0)\n",
    "    sgc_accuracy = correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lke_tJO6XVVe"
   },
   "source": [
    "### 2.9) Training of GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGx2k9qH171D",
    "outputId": "8bd25732-0432-47a9-97e3-fd2ae9455788"
   },
   "outputs": [],
   "source": [
    "print(\"=== TRAINING GCN ===\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Record the start time of the epoch\n",
    "    epoch_start_time = perf_counter()\n",
    "\n",
    "    gcn.train()\n",
    "    gcn.zero_grad()\n",
    "    output = gcn(cora_train_A, cora_train_X)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(output, cora_train_y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    gcn_optimizer.step()\n",
    "\n",
    "    # Record the end time of the epoch\n",
    "    epoch_end_time = perf_counter()\n",
    "\n",
    "    # Calculate the time taken for the epoch\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # Store the epoch time in the list\n",
    "    gcn_epoch_times.append(epoch_time)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Time: {epoch_time:.4f} seconds\")\n",
    "    gcn_loss_values.append(loss)\n",
    "\n",
    "# Calculate the average time per epoch\n",
    "gcn_avg_time_per_epoch = sum(gcn_epoch_times) / len(gcn_epoch_times)\n",
    "gcn_training_time = sum(gcn_epoch_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIJr7gACXYtQ"
   },
   "source": [
    "### 2.10) Testing of GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbKlY_aHzI80"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gcn.eval()\n",
    "    output = gcn(cora_test_A, cora_test_X)\n",
    "\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "    correct = (predicted == cora_test_y).sum().item()\n",
    "    total = cora_test_y.size(0)\n",
    "    gcn_accuracy = correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SgnUxxpXasm"
   },
   "source": [
    "### 2.11) Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "-Hf6N3wFDZGA",
    "outputId": "e5945704-9d64-4d7a-a6e8-2df994a53b8e"
   },
   "outputs": [],
   "source": [
    "# Values for plotting\n",
    "avg_time_per_epoch = [sgc_avg_time_per_epoch, gcn_avg_time_per_epoch]\n",
    "total_training_time = [sgc_training_time, gcn_training_time]\n",
    "accuracy = [sgc_accuracy, gcn_accuracy]\n",
    "\n",
    "# Labels for the bars\n",
    "labels = ['SGC', 'GCN']\n",
    "\n",
    "# Plotting average time per epoch\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(labels, avg_time_per_epoch, color=['blue', 'green'], alpha=.7)\n",
    "plt.title('Average Time per Epoch')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "# Plotting total training time\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(labels, total_training_time, color=['blue', 'green'], alpha=.7)\n",
    "plt.title('Total Training Time')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "# Plotting test accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(labels, accuracy, color=['blue', 'green'], alpha=.7)\n",
    "plt.title('Test Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(f\"Average time per Epoch for SGC: {sgc_avg_time_per_epoch:.4f} seconds\")\n",
    "print(f\"Average time per Epoch for SGC: {gcn_avg_time_per_epoch:.4f} seconds\")\n",
    "print()\n",
    "print(f\"Total training time for SGC: {sgc_training_time:.4f} seconds\")\n",
    "print(f\"Total training time for SGC (including precomputation): {sgc_training_time + precomputing_time:.4f} seconds\")\n",
    "print(f\"Total training time for GCN: {gcn_training_time:.4f} seconds\")\n",
    "print()\n",
    "print(f\"SGC Test Acc: {sgc_accuracy}\")\n",
    "print(f\"GCN Test Acc: {gcn_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cr6irixqXxDk"
   },
   "source": [
    "### 2.12) Visualization of Node Embeddings\n",
    "\n",
    "For visualization, we make use of [**TSNE**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to embed our 7-dimensional node embeddings onto a 2D plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcRdwXA2JOlA"
   },
   "outputs": [],
   "source": [
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrEuvnZoZeFG"
   },
   "source": [
    "Let's visualize the node embeddings of our **untrained** GCN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "JBH0dgVpKYUL",
    "outputId": "3771fdac-0b20-4bf2-eb73-8c6ad0c88eb7"
   },
   "outputs": [],
   "source": [
    "untrained_gcn = GCN_NC(num_feats, hidden_dim, num_classes)\n",
    "\n",
    "# Use the untrained model\n",
    "random_preds = untrained_gcn(cora_test_A, cora_test_X)\n",
    "\n",
    "visualize(random_preds, cora_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNuBCwMbZiXn"
   },
   "source": [
    "Now, let's use the GCN that we **trained** in section 2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "QdI9MoCtZDq8",
    "outputId": "9287d8ad-0afc-42f2-9351-ac9a5a0ef32e"
   },
   "outputs": [],
   "source": [
    "trained_gcn = gcn\n",
    "\n",
    "# Use the trained model\n",
    "preds = trained_gcn(cora_test_A, cora_test_X)\n",
    "\n",
    "visualize(preds, cora_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBmgJLBUN3Tz"
   },
   "source": [
    "## 3) Attention Aggregation for GCNs (Exercise)\n",
    "As you know, there are different aggrgetation methods for GCNs. Until know, we only used Kipf normalization in the form of $\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$.\n",
    "\n",
    "In this part, you are going to implement **attention based aggregation**.\n",
    "\n",
    "First, you need to apply dot product attention.\n",
    "\n",
    "<img src=\"figures/3_1.jpg\" width=\"400\" />\n",
    "\n",
    "\n",
    "Then, you need to implement the following graph attention network\n",
    "\n",
    "<img src=\"figures/3_2.jpg\" width=\"400\" />\n",
    "\n",
    "You can visit the [Lecture 3.5 Global and local aggregation methods](https://www.youtube.com/watch?v=zRmzVkidkqA).\n",
    "\n",
    "You can use the Cora dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_NC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(GCN_NC, self).__init__()\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.conv1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNLayer(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        H1 = self.conv1(X, A)\n",
    "        H1 = F.relu(H1)\n",
    "        H2 = self.conv2(H1, A)\n",
    "\n",
    "        return H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TRAINING GCN ===\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Record the start time of the epoch\n",
    "    epoch_start_time = perf_counter()\n",
    "\n",
    "    gcn.train()\n",
    "    gcn.zero_grad()\n",
    "    output = gcn(cora_train_A, cora_train_X)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(output, cora_train_y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    gcn_optimizer.step()\n",
    "\n",
    "    # Record the end time of the epoch\n",
    "    epoch_end_time = perf_counter()\n",
    "\n",
    "    # Calculate the time taken for the epoch\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # Store the epoch time in the list\n",
    "    gcn_epoch_times.append(epoch_time)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Time: {epoch_time:.4f} seconds\")\n",
    "    gcn_loss_values.append(loss)\n",
    "\n",
    "# Calculate the average time per epoch\n",
    "gcn_avg_time_per_epoch = sum(gcn_epoch_times) / len(gcn_epoch_times)\n",
    "gcn_training_time = sum(gcn_epoch_times)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
